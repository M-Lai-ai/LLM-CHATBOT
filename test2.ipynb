{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b92e0cf7-7ddf-478d-9fc5-c8df42fa50fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"none\"  # Cache les résultats dans le notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53653d41-9d24-488c-b937-876c5cb3c256",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Optional, List, Dict, Union, Any\n",
    "from datetime import datetime\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "class GlobalAIAssistant:\n",
    "    def __init__(\n",
    "        self,\n",
    "        llm_provider: str = \"openai\",  # openai, anthropic, cohere, or mistral\n",
    "        embedding_provider: str = \"openai\",  # openai, cohere, mistral, or voyage\n",
    "        llm_config: Dict[str, Any] = None,\n",
    "        embedding_config: Dict[str, Any] = None,\n",
    "        rerank_config: Dict[str, Any] = None,\n",
    "        system_prompt: str = \"You are a helpful assistant.\",\n",
    "        context_window: int = 5,\n",
    "        similar_chunks: int = 3,\n",
    "        rerank_top_k: int = 3,\n",
    "        query_history_window: int = 2,  # Nombre de questions précédentes à utiliser\n",
    "        verbose: bool = True,\n",
    "        show_context: bool = False\n",
    "    ):\n",
    "        self.llm_provider = llm_provider.lower()\n",
    "        self.embedding_provider = embedding_provider.lower()\n",
    "        self.context_window = context_window\n",
    "        self.similar_chunks = similar_chunks\n",
    "        self.rerank_top_k = rerank_top_k\n",
    "        self.query_history_window = query_history_window\n",
    "        self.verbose = verbose\n",
    "        self.show_context = show_context\n",
    "        \n",
    "        # Initialize components\n",
    "        self.llm = self._initialize_llm(llm_config or {})\n",
    "        self.embedding_model = self._initialize_embedding(embedding_config or {})\n",
    "        self.reranker = self._initialize_reranker(rerank_config or {})\n",
    "        self.chatbot = self._initialize_chatbot(system_prompt)\n",
    "        \n",
    "        # Initialize histories\n",
    "        self.visible_history: List[Dict] = []\n",
    "        self.real_history: List[Dict] = []\n",
    "        \n",
    "        # Context markers\n",
    "        self.context_start_marker = \"### Relevant Context ###\\n\"\n",
    "        self.context_end_marker = \"\\n### End Context ###\\n\"\n",
    "        self.question_marker = \"\\nQuestion: \"\n",
    "\n",
    "    def _initialize_llm(self, config: Dict[str, Any]):\n",
    "        \"\"\"Initialize the LLM based on provider\"\"\"\n",
    "        if self.llm_provider == \"openai\":\n",
    "            from openai import OpenAI_LLM\n",
    "            return OpenAI_LLM(**config)\n",
    "        elif self.llm_provider == \"anthropic\":\n",
    "            from anthropic import Anthropic_LLM\n",
    "            return Anthropic_LLM(**config)\n",
    "        elif self.llm_provider == \"cohere\":\n",
    "            from cohere import Cohere_LLM\n",
    "            return Cohere_LLM(**config)\n",
    "        elif self.llm_provider == \"mistral\":\n",
    "            from mistral import Mistral_LLM\n",
    "            return Mistral_LLM(**config)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported LLM provider: {self.llm_provider}\")\n",
    "\n",
    "    def _initialize_embedding(self, config: Dict[str, Any]):\n",
    "        \"\"\"Initialize the embedding model based on provider\"\"\"\n",
    "        if self.embedding_provider == \"openai\":\n",
    "            from openai_embedding import OpenAI_Embedding\n",
    "            return OpenAI_Embedding(**config)\n",
    "        elif self.embedding_provider == \"cohere\":\n",
    "            from cohere_embedding import Cohere_Embedding\n",
    "            return Cohere_Embedding(**config)\n",
    "        elif self.embedding_provider == \"mistral\":\n",
    "            from mistral_embedding import Mistral_Embedding\n",
    "            return Mistral_Embedding(**config)\n",
    "        elif self.embedding_provider == \"voyage\":\n",
    "            from voyage_embedding import Voyage_Embedding\n",
    "            return Voyage_Embedding(**config)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported embedding provider: {self.embedding_provider}\")\n",
    "\n",
    "    def _initialize_reranker(self, config: Dict[str, Any]):\n",
    "        \"\"\"Initialize the Voyage reranker\"\"\"\n",
    "        from voyage_rerank import Voyage_Rerank\n",
    "        return Voyage_Rerank(top_k=self.rerank_top_k, **config)\n",
    "\n",
    "    def _initialize_chatbot(self, system_prompt: str):\n",
    "        \"\"\"Initialize the chatbot based on LLM provider\"\"\"\n",
    "        if self.llm_provider == \"openai\":\n",
    "            from openai import OpenAI_Chatbot\n",
    "            return OpenAI_Chatbot(self.llm, system_prompt=system_prompt, verbose=self.verbose)\n",
    "        elif self.llm_provider == \"anthropic\":\n",
    "            from anthropic import Anthropic_Chatbot\n",
    "            return Anthropic_Chatbot(self.llm, system_prompt=system_prompt, verbose=self.verbose)\n",
    "        elif self.llm_provider == \"cohere\":\n",
    "            from cohere import Cohere_Chatbot\n",
    "            return Cohere_Chatbot(self.llm, system_prompt=system_prompt, verbose=self.verbose)\n",
    "        elif self.llm_provider == \"mistral\":\n",
    "            from mistral import Mistral_Chatbot\n",
    "            return Mistral_Chatbot(self.llm, system_prompt=system_prompt, verbose=self.verbose)\n",
    "\n",
    "    def _get_recent_queries(self, current_query: str) -> str:\n",
    "        \"\"\"Get recent user queries for context\"\"\"\n",
    "        recent_queries = []\n",
    "        \n",
    "        # Parcourir l'historique réel pour trouver les questions originales\n",
    "        for msg in self.real_history:\n",
    "            if msg[\"role\"] == \"user\":\n",
    "                recent_queries.append(msg[\"original_query\"])\n",
    "        \n",
    "        # Prendre les n dernières questions + la question actuelle\n",
    "        recent_queries = recent_queries[-self.query_history_window:]\n",
    "        recent_queries.append(current_query)\n",
    "        \n",
    "        # Concaténer les questions\n",
    "        return \" \".join(recent_queries)\n",
    "\n",
    "    def _remove_context_from_message(self, message: str) -> str:\n",
    "        \"\"\"Remove context section from message\"\"\"\n",
    "        if self.context_start_marker in message and self.context_end_marker in message:\n",
    "            pattern = f\"{self.context_start_marker}.*?{self.context_end_marker}\"\n",
    "            message = re.sub(pattern, \"\", message, flags=re.DOTALL)\n",
    "            # Remove Question marker if present\n",
    "            message = message.replace(self.question_marker, \"\")\n",
    "        return message.strip()\n",
    "\n",
    "\n",
    "    def _get_similar_chunks(self, query: str, index_name: str) -> List[str]:\n",
    "        \"\"\"Get similar chunks from the embedding index and rerank them\"\"\"\n",
    "        try:\n",
    "            # Get initial results from embedding search\n",
    "            results = self.embedding_model.search(index_name, query, k=self.similar_chunks)\n",
    "            initial_chunks = [result[\"text\"] for result in results]\n",
    "            \n",
    "            if not initial_chunks:\n",
    "                return []\n",
    "            \n",
    "            # Rerank the chunks\n",
    "            if self.verbose:\n",
    "                print(\"Reranking retrieved chunks...\")\n",
    "            \n",
    "            reranked_chunks = self.reranker.get_best_chunks(query, initial_chunks)\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(f\"Selected {len(reranked_chunks)} best chunks after reranking\")\n",
    "            \n",
    "            return reranked_chunks\n",
    "            \n",
    "        except Exception as e:\n",
    "            if self.verbose:\n",
    "                print(f\"Warning: Could not retrieve or rerank chunks: {e}\")\n",
    "            return []\n",
    "\n",
    "    def _format_message_with_context(self, message: str, chunks: List[str]) -> str:\n",
    "        \"\"\"Format message with retrieval context\"\"\"\n",
    "        if not chunks:\n",
    "            return message\n",
    "            \n",
    "        context = \"\\n\".join(f\"- {chunk}\" for chunk in chunks)\n",
    "        return (\n",
    "            f\"{self.context_start_marker}\"\n",
    "            f\"{context}\"\n",
    "            f\"{self.context_end_marker}\"\n",
    "            f\"{self.question_marker}{message}\"\n",
    "        )\n",
    "\n",
    "    def create_knowledge_base(self, texts: List[str], index_name: str):\n",
    "        \"\"\"Create a knowledge base from texts\"\"\"\n",
    "        try:\n",
    "            self.embedding_model.create_faiss_index(index_name, texts)\n",
    "            if self.verbose:\n",
    "                print(f\"Successfully created knowledge base: {index_name}\")\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Failed to create knowledge base: {e}\")\n",
    "\n",
    "    def update_knowledge_base(self, new_texts: List[str], index_name: str):\n",
    "        \"\"\"Update existing knowledge base with new texts\"\"\"\n",
    "        try:\n",
    "            self.embedding_model.update_index(index_name, new_texts)\n",
    "            if self.verbose:\n",
    "                print(f\"Successfully updated knowledge base: {index_name}\")\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Failed to update knowledge base: {e}\")\n",
    "\n",
    "    def load_existing_knowledge_base(\n",
    "        self,\n",
    "        chunks_file: str,\n",
    "        embeddings_file: str,\n",
    "        index_name: str\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Load an existing knowledge base from chunks.json and embeddings.npy files\n",
    "        \n",
    "        Args:\n",
    "            chunks_file: Path to the chunks JSON file\n",
    "            embeddings_file: Path to the embeddings NPY file\n",
    "            index_name: Name to give to the loaded index\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Load chunks and metadata\n",
    "            with open(chunks_file, 'r', encoding='utf-8') as f:\n",
    "                chunks_data = json.load(f)\n",
    "                \n",
    "            # Load embeddings\n",
    "            embeddings = np.load(embeddings_file)\n",
    "            \n",
    "            # Extract texts and create metadata\n",
    "            texts = [chunk[\"text\"] for chunk in chunks_data]\n",
    "            chunks_metadata = {\n",
    "                \"created_at\": datetime.now().isoformat(),\n",
    "                \"model\": self.embedding_model.model,\n",
    "                \"total_chunks\": len(texts),\n",
    "                \"embedding_dim\": embeddings.shape[1],\n",
    "                \"chunks\": [\n",
    "                    {\n",
    "                        \"id\": i,\n",
    "                        \"text\": chunk[\"text\"],\n",
    "                        \"embedding_index\": i,\n",
    "                        \"original_metadata\": chunk.get(\"metadata\", {})\n",
    "                    }\n",
    "                    for i, chunk in enumerate(chunks_data)\n",
    "                ]\n",
    "            }\n",
    "            \n",
    "            # Create FAISS index\n",
    "            index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "            index.add(embeddings)\n",
    "            \n",
    "            # Save everything using existing methods\n",
    "            self.embedding_model.save_index(name=index_name, \n",
    "                                          index=index, \n",
    "                                          chunks_metadata=chunks_metadata, \n",
    "                                          embeddings=embeddings)\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(f\"Successfully loaded knowledge base '{index_name}' from files:\")\n",
    "                print(f\"- Chunks: {chunks_file}\")\n",
    "                print(f\"- Embeddings: {embeddings_file}\")\n",
    "                print(f\"- Total chunks: {len(texts)}\")\n",
    "                print(f\"- Embedding dimension: {embeddings.shape[1]}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Failed to load knowledge base from files: {e}\")\n",
    "\n",
    "    def chat(self, message: str, index_name: Optional[str] = None) -> str:\n",
    "        \"\"\"Process user message with context management\"\"\"\n",
    "        # Garder la question originale\n",
    "        original_message = message\n",
    "        \n",
    "        # Get similar chunks if index_name is provided\n",
    "        if index_name:\n",
    "            # Obtenir le contexte de recherche avec les questions précédentes\n",
    "            search_context = self._get_recent_queries(original_message)\n",
    "            \n",
    "            # Get new retrieval context with reranking\n",
    "            similar_chunks = self._get_similar_chunks(search_context, index_name)\n",
    "            \n",
    "            # Format message with new context\n",
    "            enhanced_message = self._format_message_with_context(message, similar_chunks)\n",
    "        else:\n",
    "            enhanced_message = message\n",
    "        \n",
    "        # Get response from chatbot\n",
    "        response = self.chatbot(enhanced_message)\n",
    "        \n",
    "        # Mettre à jour les historiques\n",
    "        # Historique réel (avec contexte)\n",
    "        self.real_history.append({\n",
    "            \"role\": \"user\",\n",
    "            \"content\": enhanced_message,\n",
    "            \"original_query\": original_message,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        })\n",
    "        self.real_history.append({\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": response,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        })\n",
    "        \n",
    "        # Historique visible (sans contexte)\n",
    "        self.visible_history.append({\n",
    "            \"role\": \"user\",\n",
    "            \"content\": original_message,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        })\n",
    "        self.visible_history.append({\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": response,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        })\n",
    "        \n",
    "        return response\n",
    "\n",
    "    def get_visible_history(self) -> List[Dict]:\n",
    "        \"\"\"Get the visible conversation history\"\"\"\n",
    "        return self.visible_history\n",
    "\n",
    "    def get_real_history(self) -> List[Dict]:\n",
    "        \"\"\"Get the real conversation history (with context)\"\"\"\n",
    "        return self.real_history\n",
    "\n",
    "    def start_new_conversation(self):\n",
    "        \"\"\"Start a new conversation\"\"\"\n",
    "        self.chatbot.start_new_conversation()\n",
    "        self.visible_history = []\n",
    "        self.real_history = []\n",
    "        if self.verbose:\n",
    "            print(\"Started new conversation\")\n",
    "\n",
    "    def save_conversation(self, conversation_id: str):\n",
    "        \"\"\"Save both visible and real conversation histories\"\"\"\n",
    "        conversation_data = {\n",
    "            \"conversation_id\": conversation_id,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"llm_provider\": self.llm_provider,\n",
    "            \"embedding_provider\": self.embedding_provider,\n",
    "            \"visible_history\": self.visible_history,\n",
    "            \"real_history\": self.real_history,\n",
    "            \"metadata\": {\n",
    "                \"context_window\": self.context_window,\n",
    "                \"similar_chunks\": self.similar_chunks,\n",
    "                \"rerank_top_k\": self.rerank_top_k,\n",
    "                \"query_history_window\": self.query_history_window\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Créer le dossier de conversations s'il n'existe pas\n",
    "        conversation_dir = os.path.join(\"conversations\", self.llm_provider)\n",
    "        os.makedirs(conversation_dir, exist_ok=True)\n",
    "        \n",
    "        # Sauvegarder les conversations\n",
    "        file_path = os.path.join(conversation_dir, f\"{conversation_id}.json\")\n",
    "        with open(file_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(conversation_data, f, indent=2, ensure_ascii=False)\n",
    "            \n",
    "        if self.verbose:\n",
    "            print(f\"Saved conversation to: {file_path}\")\n",
    "\n",
    "    def load_conversation(self, conversation_id: str):\n",
    "        \"\"\"Load both visible and real conversation histories\"\"\"\n",
    "        file_path = os.path.join(\"conversations\", self.llm_provider, f\"{conversation_id}.json\")\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "                self.visible_history = data[\"visible_history\"]\n",
    "                self.real_history = data[\"real_history\"]\n",
    "                \n",
    "                # Vérifier la compatibilité des providers\n",
    "                if data[\"llm_provider\"] != self.llm_provider:\n",
    "                    print(f\"Warning: Conversation was created with {data['llm_provider']} provider, \"\n",
    "                          f\"but current provider is {self.llm_provider}\")\n",
    "                \n",
    "                if self.verbose:\n",
    "                    print(f\"Loaded conversation: {conversation_id}\")\n",
    "                    print(f\"Total messages: {len(self.visible_history)}\")\n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError(f\"Conversation {conversation_id} not found at {file_path}\")\n",
    "\n",
    "    def list_conversations(self) -> List[str]:\n",
    "        \"\"\"List all conversations for current provider\"\"\"\n",
    "        conversation_dir = os.path.join(\"conversations\", self.llm_provider)\n",
    "        if not os.path.exists(conversation_dir):\n",
    "            return []\n",
    "        \n",
    "        conversations = [f.replace('.json', '') \n",
    "                        for f in os.listdir(conversation_dir) \n",
    "                        if f.endswith('.json')]\n",
    "        return sorted(conversations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe2fe591-f51d-4fa3-9ef0-8632e60a79bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_config = {\n",
    "        \"model\": \"gpt-4o-mini\", \n",
    "        \"temperature\": 0,\n",
    "        \"max_tokens\": 15000,\n",
    "        \"stream\": True\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bb539d8-0128-4469-a590-5efbdafac9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_config = {\n",
    "        \"model\": \"text-embedding-3-large\"  # Doit correspondre au modèle utilisé pour créer les embeddings\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10d62d41-900c-4489-b37e-d71d16fee0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rerank_config = {\n",
    "        \"model\": \"rerank-2\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcd24d9b-7fd5-491d-96bf-42faf6107712",
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant = GlobalAIAssistant(\n",
    "        llm_provider=\"openai\",\n",
    "        embedding_provider=\"openai\",  # Doit correspondre au provider utilisé pour créer les embeddings\n",
    "        llm_config=llm_config,\n",
    "        embedding_config=embedding_config,\n",
    "        rerank_config=rerank_config,\n",
    "        system_prompt=\"Tu es un assistant virtuel pour la Faculté des sciences de l'administration de l'Université Laval. Ton rôle est d'assister les étudiants potentiels, actuels, les membres du personnel, et toute personne qui visite le site web de la Faculté. Tu dois fournir des informations détaillées et précises sur les programmes offerts, guider les étudiants dans leurs choix académiques en fonction de leurs besoins, et les renseigner sur les services administratifs, les opportunités de recherche, les informations sur les employés, et les événements organisés par la Faculté. Sois toujours accueillant, clair, et fourni des réponses concises mais suffisamment informatives pour répondre aux besoins des utilisateurs. Pour répondre aux questions, tu dois te baser uniquement sur le contexte qui t'est fourni. Si possible présente les liens vers le site web des infos que tu présente. \",\n",
    "        context_window=10,\n",
    "        similar_chunks=150,\n",
    "        rerank_top_k=20,\n",
    "        query_history_window=2,  # Utiliser les 2 dernières questions pour la recherche\n",
    "        verbose=False,\n",
    "        show_context=False\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91970b27-62f2-4ad8-b028-f3591fc2df35",
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant.load_existing_knowledge_base(\n",
    "        chunks_file=\"/Users/simon-pierreboucher/Downloads/LLM-CHATBOT-main-3/chunks.json\",\n",
    "        embeddings_file=\"/Users/simon-pierreboucher/Downloads/LLM-CHATBOT-main-3/embeddings.npy\",\n",
    "        index_name=\"fsa\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "338c5e64-9748-488f-9116-e5bfb21e0c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "response=assistant.chat(\n",
    "        \"Quel département y a t'il a la FSA\",\n",
    "        index_name=\"fsa\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51912fb6-422f-4afd-bbc0-4a416d88387e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Question:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Quel département y a t'il a la FSA"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Response:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "La Faculté des sciences de l'administration (FSA) de l'Université Laval regroupe cinq départements et une école, tous situés dans le pavillon Palasis-Prince. Voici la liste des départements et de l'école :\n",
       "\n",
       "1. **Département de finance, assurance et immobilier** - [En savoir plus](https://www.fsa.ulaval.ca/faculte/departements-ecole/fai)\n",
       "2. **Département de management** - [En savoir plus](https://www.fsa.ulaval.ca/faculte/departements-ecole/mng)\n",
       "3. **Département de marketing** - [En savoir plus](https://www.fsa.ulaval.ca/faculte/departements-ecole/mrk)\n",
       "4. **Département d’opérations et systèmes de décision** - [En savoir plus](https://www.fsa.ulaval.ca/faculte/departements-ecole/osd)\n",
       "5. **Département de systèmes d’information organisationnels** - [En savoir plus](https://www.fsa.ulaval.ca/faculte/departements-ecole/sio)\n",
       "6. **École de comptabilité** - [En savoir plus](https://www.fsa.ulaval.ca/faculte/departements-ecole/ctb)\n",
       "\n",
       "Pour plus d'informations sur chaque département, vous pouvez consulter les liens fournis."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "question = assistant.visible_history[-2][\"content\"]\n",
    "display(Markdown(f\"**Question:**\"))\n",
    "display(Markdown(question))\n",
    "display(Markdown(f\"**Response:**\"))\n",
    "display(Markdown(response))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d7f7b2a-8dd5-426e-8722-16b50a478594",
   "metadata": {},
   "outputs": [],
   "source": [
    "response=assistant.chat(\n",
    "        \"Qui est directeur du département de finance\",\n",
    "        index_name=\"fsa\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec111ac9-ec3e-421a-8727-b59c45c1f014",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Question:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Qui est directeur du département de finance"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Response:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Le directeur du Département de finance, assurance et immobilier (FAI) à la FSA ULaval est **Charles-Olivier Amédée-Manesme**. Il a été nommé à ce poste le 27 novembre 2023, après avoir occupé le rôle de directeur par intérim depuis le 16 juin 2023. Pour plus d'informations, vous pouvez consulter [cette page](https://www.fsa.ulaval.ca/nouvelles/charles-olivier-amedee-manesme-nomme-directeur-du-departement-de-finance-et-assurance-et-immobilier/)."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "question = assistant.visible_history[-2][\"content\"]\n",
    "display(Markdown(f\"**Question:**\"))\n",
    "display(Markdown(question))\n",
    "display(Markdown(f\"**Response:**\"))\n",
    "display(Markdown(response))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a0720e3-5ff3-483d-b2fc-7f74597dc5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "response=assistant.chat(\n",
    "        \"Peux m'en dire plus sur lui\",\n",
    "        index_name=\"ouellet\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "759f509c-943e-4146-a1ba-d4e53829dbd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Question:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Peux m'en dire plus sur lui"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Response:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Charles-Olivier Amédée-Manesme est professeur au Département de finance, assurance et immobilier (FAI) de la Faculté des sciences de l'administration (FSA) de l'Université Laval. Il est spécialisé dans le domaine de l'immobilier. Avant de rejoindre le milieu universitaire, il a travaillé pour BNP Paribas Real Estate en investissement non coté pendant quatre ans.\n",
       "\n",
       "Ses recherches portent sur plusieurs sujets, notamment la quantification des risques en immobilier, les marchés immobiliers, les investissements illiquides et l'économie urbaine. Charles-Olivier détient un doctorat en finance de l'Université de Cergy-Pontoise, ainsi qu'un diplôme d'ingénieur de l'École spéciale des travaux publics de Paris. Il a également obtenu un master de recherche en économie de l'École normale supérieure de Cachan (Université La Sorbonne Paris I) et un mastère spécialisé en techniques financières de l'ESSEC Business School.\n",
       "\n",
       "Pour plus d'informations sur ses travaux et son parcours, vous pouvez consulter la page de son profil sur le site de la FSA ULaval."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "question = assistant.visible_history[-2][\"content\"]\n",
    "display(Markdown(f\"**Question:**\"))\n",
    "display(Markdown(question))\n",
    "display(Markdown(f\"**Response:**\"))\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf1bc97b-f1bc-45a0-a2c2-824eab75ea11",
   "metadata": {},
   "outputs": [],
   "source": [
    "response=assistant.chat(\n",
    "        \"Quel est l'adresse de la FSA ? \",\n",
    "        index_name=\"fsa\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "87244dae-8933-45bf-8f3e-f06f3d4efde2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Question:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Quel est l'adresse de la FSA ? "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Response:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "L'adresse de la Faculté des sciences de l'administration (FSA) de l'Université Laval est :\n",
       "\n",
       "**Faculté des sciences de l'administration**  \n",
       "Pavillon Palasis-Prince  \n",
       "2325, rue de la Terrasse  \n",
       "Québec (Québec)  \n",
       "G1V 0A6  \n",
       "CANADA\n",
       "\n",
       "Pour toute information supplémentaire, vous pouvez également les contacter par téléphone au 418 656-2584."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "question = assistant.visible_history[-2][\"content\"]\n",
    "display(Markdown(f\"**Question:**\"))\n",
    "display(Markdown(question))\n",
    "display(Markdown(f\"**Response:**\"))\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad21061f-4a8f-4a5a-a663-80d36e0a8269",
   "metadata": {},
   "outputs": [],
   "source": [
    "response=assistant.chat(\n",
    "        \"Donne moi les publications de Richard Luger\",\n",
    "        index_name=\"fsa\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b9efcbcc-5780-4526-9e47-7c38abb8e15e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Question:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Donne moi les publications de Richard Luger"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Response:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Voici une sélection des publications de Richard Luger :\n",
       "\n",
       "### Articles\n",
       "1. **Luger, R. (2024)**. Regularizing stock return covariance matrices via multiple testing of correlations. *Journal of Econometrics*, 105753. DOI : [10.1016/j.jeconom.2024.105753](https://doi.org/10.1016/j.jeconom.2024.105753)\n",
       "\n",
       "2. **Fu, H., & Luger, R. (2022)**. Multiple testing of the forward rate unbiasedness hypothesis across currencies. *Journal of Empirical Finance*, 68, 232-245. DOI : [10.1016/j.jempfin.2022.07.005](https://doi.org/10.1016/j.jempfin.2022.07.005)\n",
       "\n",
       "3. **Gungor, S., & Luger, R. (2021)**. Exact inference in long-horizon predictive quantile regressions with an application to stock returns. *Journal of Financial Econometrics*, 19(4), 746-788. DOI : [10.1093/jjfinec/nbz017](https://doi.org/10.1093/jjfinec/nbz017)\n",
       "\n",
       "4. **Gungor, S., & Luger, R. (2020)**. Small-sample tests for stock return predictability with possibly non-stationary regressors and GARCH-type effects. *Journal of Econometrics*, 218(2), 750-770. DOI : [10.1016/j.jeconom.2020.04.037](https://doi.org/10.1016/j.jeconom.2020.04.037)\n",
       "\n",
       "5. **Liu, X., & Luger, R. (2018)**. Markov-switching quantile autoregression: A Gibbs sampling approach. *Studies in Nonlinear Dynamics and Econometrics*, 22(2), 1-33. DOI : [10.1515/snde-2016-0078](https://doi.org/10.1515/snde-2016-0078)\n",
       "\n",
       "6. **Dufour, J. M., & Luger, R. (2017)**. Identification-robust moment-based tests for Markov switching in autoregressive models. *Econometric Reviews*, 36(6-9), 713-727. DOI : [10.1080/07474938.2017.1307548](https://doi.org/10.1080/07474938.2017.1307548)\n",
       "\n",
       "7. **Gungor, S., & Luger, R. (2016)**. Multivariate Tests of Mean-Variance Efficiency and Spanning With a Large Number of Assets and Time-Varying Covariances. *Journal of Business & Economic Statistics*, 34(2), 161-175. DOI : [10.1080/07350015.2015.1019510](https://doi.org/10.1080/07350015.2015.1019510)\n",
       "\n",
       "8. **Liu, X., & Luger, R. (2015)**. Unfolded GARCH Models. *Journal of Economic Dynamics and Control*, 58, 186-217. DOI : [10.1016/j.jedc.2015.06.007](https://doi.org/10.1016/j.jedc.2015.06.007)\n",
       "\n",
       "### Autres publications\n",
       "- Richard Luger a également contribué à plusieurs conférences et a publié des travaux sur des sujets variés liés à la finance quantitative, à l'économétrie financière et à la gestion des risques.\n",
       "\n",
       "Pour plus de détails sur ses publications, vous pouvez consulter son profil sur le site de la FSA ULaval [ici](https://www.fsa.ulaval.ca/personnel-expert/richard-luger)."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "question = assistant.visible_history[-2][\"content\"]\n",
    "display(Markdown(f\"**Question:**\"))\n",
    "display(Markdown(question))\n",
    "display(Markdown(f\"**Response:**\"))\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e49c8e8c-9981-4c7a-a9c6-02f2ce4a3eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "response=assistant.chat(\n",
    "        \"Donne moi les étudiants de doctorat qu'il supervise en ce moment\",\n",
    "        index_name=\"fsa\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dbbebdd7-efa8-4ab2-953f-121451333104",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Question:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Donne moi les étudiants de doctorat qu'il supervise en ce moment"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Response:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Richard Luger supervise actuellement le doctorant suivant :\n",
       "\n",
       "- **Kouakou Arsène Brou** (Automne 2019 - ...)\n",
       "\n",
       "Pour plus d'informations sur ses activités et ses publications, vous pouvez consulter son profil sur le site de la FSA ULaval [ici](https://www.fsa.ulaval.ca/personnel-expert/richard-luger)."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "question = assistant.visible_history[-2][\"content\"]\n",
    "display(Markdown(f\"**Question:**\"))\n",
    "display(Markdown(question))\n",
    "display(Markdown(f\"**Response:**\"))\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e447b608-1a3f-4194-b30e-3676975e23e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant = GlobalAIAssistant(\n",
    "        llm_provider=\"openai\",\n",
    "        embedding_provider=\"openai\",  # Doit correspondre au provider utilisé pour créer les embeddings\n",
    "        llm_config=llm_config,\n",
    "        embedding_config=embedding_config,\n",
    "        rerank_config=rerank_config,\n",
    "        system_prompt=\"Tu es un assistant virtuel pour la Faculté des sciences de l'administration de l'Université Laval. Ton rôle est d'assister les étudiants potentiels, actuels, les membres du personnel, et toute personne qui visite le site web de la Faculté. Tu dois fournir des informations détaillées et précises sur les programmes offerts, guider les étudiants dans leurs choix académiques en fonction de leurs besoins, et les renseigner sur les services administratifs, les opportunités de recherche, les informations sur les employés, et les événements organisés par la Faculté. Sois toujours accueillant, clair, et fourni des réponses concises mais suffisamment informatives pour répondre aux besoins des utilisateurs. Pour répondre aux questions, tu dois te baser uniquement sur le contexte qui t'est fourni. Si possible présente les liens vers le site web des infos que tu présente. \",\n",
    "        context_window=10,\n",
    "        similar_chunks=150,\n",
    "        rerank_top_k=20,\n",
    "        query_history_window=2,  # Utiliser les 2 dernières questions pour la recherche\n",
    "        verbose=False,\n",
    "        show_context=False\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6d826cd8-ac5e-4815-827f-1dba40de68e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "response=assistant.chat(\n",
    "        \"Donne moi des détaille sur Simon-Pierre Boucher\",\n",
    "        index_name=\"fsa\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9ae79d08-42fc-4938-bacc-546e1ffe735a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Question:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Donne moi des détaille sur Simon-Pierre Boucher"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Response:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Simon-Pierre Boucher est un chercheur et professeur associé au Département de finance, assurance et immobilier de la Faculté des sciences de l'administration de l'Université Laval. Voici quelques détails sur son parcours et ses contributions :\n",
       "\n",
       "### Formation\n",
       "- **Doctorat en sciences de l'administration** - Finance et assurance (en cours) à l'Université Laval.\n",
       "- **Maîtrise ès Science** en Finance, assurance et immobilier (M. Sc.) à l'Université Laval.\n",
       "- **Baccalauréat en administration des affaires** avec spécialisation en Finance (B.A.A.) à l'Université Laval.\n",
       "\n",
       "### Publications\n",
       "Simon-Pierre Boucher a participé à plusieurs communications dans des conférences, notamment :\n",
       "- **Gagnon, M.-H., Boucher, S.-P., & Power, G. (2022)**. \"Has financialization changed the impact of macro announcements on US commodity markets?\" présenté à l'EFS 2022.\n",
       "\n",
       "### Encadrement étudiant\n",
       "Il a été impliqué dans l'encadrement d'étudiants au niveau du doctorat et de la maîtrise, agissant comme directeur ou codirecteur de recherche pour plusieurs étudiants.\n",
       "\n",
       "### Contributions à la discipline\n",
       "- Membre de divers comités d'évaluation, tels que le comité ÉducÉpargne et le Comité de direction sur la qualité de l'IQPF.\n",
       "- Membre du Conseil facultaire de la FSA (2021 - 2023).\n",
       "\n",
       "### Implications professionnelles\n",
       "Simon-Pierre Boucher a également collaboré avec des professionnels dans le domaine de la finance et a animé des formations sur des sujets liés à la planification financière et aux enjeux humains dans le transfert d'entreprise.\n",
       "\n",
       "### Prix et distinctions\n",
       "Il a reçu plusieurs distinctions, dont la Médaille de la recherche pour plusieurs années consécutives.\n",
       "\n",
       "Pour plus d'informations, vous pouvez consulter son profil sur le site de la [Faculté des sciences de l'administration de l'Université Laval](https://www.fsa.ulaval.ca/communaute-doctorale/simon-pierre-boucher)."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "question = assistant.visible_history[-2][\"content\"]\n",
    "display(Markdown(f\"**Question:**\"))\n",
    "display(Markdown(question))\n",
    "display(Markdown(f\"**Response:**\"))\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e34e04fe-a426-4952-af9c-1db11f9b806d",
   "metadata": {},
   "outputs": [],
   "source": [
    "response=assistant.chat(\n",
    "        \"Tu es sur qu'Il est professeur\",\n",
    "        index_name=\"fsa\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "45d0b1ac-827f-4d75-801d-2344f5e8f798",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Question:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Tu es sur qu'Il est professeur"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Response:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Oui, Simon-Pierre Boucher est professeur associé au Département de finance, assurance et immobilier de la Faculté des sciences de l'administration de l'Université Laval. Il est également impliqué dans la recherche et l'encadrement d'étudiants au niveau du doctorat et de la maîtrise. Pour plus de détails, vous pouvez consulter son profil sur le site de la [Faculté des sciences de l'administration de l'Université Laval](https://www.fsa.ulaval.ca/communaute-doctorale/simon-pierre-boucher)."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "question = assistant.visible_history[-2][\"content\"]\n",
    "display(Markdown(f\"**Question:**\"))\n",
    "display(Markdown(question))\n",
    "display(Markdown(f\"**Response:**\"))\n",
    "display(Markdown(response))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
